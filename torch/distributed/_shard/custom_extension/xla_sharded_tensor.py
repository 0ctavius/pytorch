# NOTE: this is just example prototype
import torch
from torch.distributed._shard.sharded_tensor.interface import ShardedTensorInterface
from torch.distributed._shard.sharding_spec import ShardingSpec
from torch.utils._pytree import tree_map


from dataclasses import dataclass
from typing import List, Tuple, Iterator
import contextlib

@contextlib.contextmanager
def no_dispatch() -> Iterator[None]:
    guard = torch._C._DisableTorchDispatch()  # type: ignore[attr-defined]
    try:
        yield
    finally:
        del guard

@dataclass
class XLAShardingSpec(ShardingSpec):
    # Inheriting PT-D base ShardingSpec
    # A logical device topology, each element describes
    # a number of devices in the corresponding axis.
    # NOTE: we could use more specific device-rank mapping, e.g., ShardingSpec,
    # if needed. The change shouldn't be difficult, or create another constructor.
    mesh_shape: Tuple[int]   # TODO: create a wrapper for named axes
    # Specifies how each input rank is sharded (index to mesh_shape)
    # or replicated (None). For example, we can shard an 8x10 tensor
    # 4-way row-wise, and replicate column-wise.
    # >> input = torch.randn(8, 10)
    # >> mesh_shape = (4, 2)
    # >> assert np.prod(mesh_shape) == xm.xrt_world_size()
    # >> partition_spec = (0, None)
    # >> assert len(input.shape) == len(partition_spec)
    partition_spec: Tuple[int, None]


@dataclass
class XLAShard:
    data: torch.Tensor
    rank: int

# torch_xla.distributed.xla_sharding
class XLAShardedTensor(ShardedTensorInterface):
    """
    A wrapper around `ShardedTensorInterface` with sharding annotation
    for XLA SPMD auto-sharding. The wrapped tensors are unwrapped
    for IR tracing and converted to HLO graph with sharding annotations;
    XLA SPMDPartitioner takes a pass, propagating and injecting collectives
    to the graph before compilation.
    """
    def __init__(
        self,
        sharding_spec: XLAShardingSpec,
        *size,
        global_tensor,
        **kwargs
    ):
        # XLAShardedTensor behaves like a unpartitioned,
        # combined tensor on the host machine. When user annotates,
        # this is simply set to the input tensor. When an XLA partitioned
        # output tensor returns (or sharding propagated intermediate tensors)
        # as XLAShardedTensor, the backend gathers global data across devices
        # and materialize and set `global_tensor` on the host; the actual device
        # data still remain on individual device as sharded or replicated.
        # Note: we should drop this reference, and force all gather on each
        # access.
        self._global_tensor = global_tensor
        self._sharding_spec = sharding_spec
        self._size = size
        # Shards on the devices are materialized/available after the lazy
        # execution of the SPMDPartitioned HLO graph; otherwise,
        # local_shards is set to `None`. Each XLAShard points to
        # torch.Tensor (xla::device_data).
        # Note: we can consider returning a callback or even define
        # sharding at XLAShardedTensor construction after pjrt migration.
        self._local_shards: List[XLAShard] = None

    @property
    def size(self) -> torch.Size:
        "Returns the (global) shape of this tensor"
        pass


    def __repr__(self):
        return f"XLAShardedTensor({self._global_tensor})"

    @classmethod
    def __torch__dispatch__(cls, func, types, args=(), kwargs=None):
        """
        Approach 1
        This may not be an accurate use of __torch__dispatch__, but the idea is
        to send the unwrapped tensor with sharding annotation to XLA backend

        use the following or similar operators to check if at::Tensor is sharded,
        retrieve the sharding annotations, etc.

        e.g.,)
        at::Tensor XLATensor::add(const at::Tensor& rhs) {
            if(aten::is_sharded(rhs)) {
            # rhs be a XLAShardedTensor
            auto kwargs = aten::get_shards_metadata(rhs)
            }
        }
        """
        def unwrap(t):
            return t.__global_tensor if isinstance(t, XLAShardedTensor) else t

        def wrap(t):
            return XLAShardedTensor(t) if isinstance(t, torch.Tensor) else t

        # no_dispatch is only needed if you use enable_python_mode.
        # It prevents infinite recursion.
        with no_dispatch():
            # re-dispatch to C++
            rs = tree_map(wrap, func(*tree_map(unwrap, args), **tree_map(unwrap, kwargs)))
        return rs

    @classmethod
    def __torch__dispatch__(cls, func, types, args=(), kwargs=None):
        """
        Approach 2
        This approach first use the global tensor to go to XLA backend to get
        a HLO graph/node, then right after the op generated by XLA backend,
        we pass it to SPMD partitioner to swap the node with a node with
        collectives.

        Pros:
            * no perf overhead embedded in each XLA op, only XLAShardedTensor
              involved op will do additional handling
            * no need to introduce C++ specific aten utils to check if the
              tensor is a sharded tensor/ retrive sharding information from tensor


        Cons:
            * need to have python bindings for SPMD partitioner/HLO binding
            * might need to call another SPMD partitioner pass after the entire
              graph generated to do things like fusion?

        """
        def unwrap(t):
            return t.__global_tensor if isinstance(t, XLAShardedTensor) else t

        def wrap(t):
            return XLAShardedTensor(t) if isinstance(t, torch.Tensor) else t

        # no_dispatch is only needed if you use enable_python_mode.
        # It prevents infinite recursion.
        with no_dispatch():
            # re-dispatch to C++
            rs = tree_map(wrap, func(*tree_map(unwrap, args), **tree_map(unwrap, kwargs)))

        # We know that when it calls into __torch_dispatch__, it/args must be a
        # XLAShardedTensor instead of a torch.Tensor, so no need to check is_sharded.
        #
        # By now, we should have a HLO graph (unsharded version), or node?
        # in python level, we can call SPMDPartitioner with the HLOSharding spcificiation
        # we can have a method to translate XLAShardingSpec -> HLOSharding pybind binding?
        # ASK: can you share more internals about how you want to call SPMDPartitioner

        # Assumption: SPMP partitioner/ HLO Sharding spec both have a python binding
        XLA.SPMDPartitioner(hlo_graph, hlo_sharding_spec)
        return rs
